{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test.ipynb","provenance":[],"mount_file_id":"1i_LZGWUyOSpNvwlJ3HrMeP5Sm46L67G6","authorship_tag":"ABX9TyPdOhFa8VpT3v4JM306zBo6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"II_FUIJVvhAc","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Setup\n","\n","%cd 'drive/My Drive/NLP_project'\n","!pip install pyvi\n","\n","from preprocessing import NLP\n","from VectorizationWord import FeatureExtraction\n","from tensorflow.keras.models import load_model\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from glob import glob\n","import numpy as np\n","from define import *\n","from gensim.models import Word2Vec\n","\n","word2vec = Word2Vec.load(WORD2VEC_PATH)\n","tfidf = pickle.load(open(TFIDF_PATH, \"rb\"))\n","tfidf_weighed = pickle.load(open(TFIDF_PATH.replace('.p','weighed.p'), \"rb\"))\n","\n","lb = pickle.load(open('features/LabelEncoder.p', \"rb\"))\n","\n","h5paths_tfidf = glob('model/bestcp_TFIDF7_*.h5')\n","model_tfidf = load_model(h5paths_tfidf[-1])\n","h5paths_w2v = glob('model/bestcp_W2V3_*.h5')\n","model_w2v = load_model(h5paths_w2v[-1])\n","h5paths_wwv = glob('model/bestcp_WWV5_*.h5')\n","model_wwv = load_model(h5paths_wwv[-1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWkwx8OWv04t","colab_type":"code","outputId":"e1b9128f-0878-40b1-ad83-5d17e0880df8","executionInfo":{"status":"ok","timestamp":1592201703946,"user_tz":-420,"elapsed":792,"user":{"displayName":"Hoàng Nguyễn Việt","photoUrl":"","userId":"10533201639481242505"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["#@title Phân loại văn bản {display-mode: \"form\"}\n","\n","Input = \"HLV Mallorca s\\u1EE3 va ch\\u1EA1m v\\u1EDBi Messi\"  #@param {type: \"string\"}\n","\n","Method = \"Weighted Word Vectors\" #@param ['TF-IDF', 'Word2Vec', 'Weighted Word Vectors']\n","#@markdown ---\n","\n","method_dict = {\n","    'TF-IDF' : 0,\n","    'Word2Vec' : 1,\n","    'Weighted Word Vectors' : 2\n","}\n","\n","VectorizationWordMethod = method_dict[Method]\n","\n","data = NLP(Input).get_text_feature()\n","\n","if VectorizationWordMethod == 0:\n","    features = tfidf.transform([data]).todense()\n","    y_pred = model_tfidf.predict(features)\n","    label = lb.inverse_transform([np.argmax(y_pred)])\n","\n","elif VectorizationWordMethod == 1:\n","    words = data.split(\" \")\n","    vec = []\n","    for i in range(40):\n","        try:\n","            vec.append(word2vec.wv[words[i]])\n","        except KeyError:\n","            vec.append(np.random.normal(0, 0.05, 300))\n","        except IndexError:\n","            vec.append(np.random.normal(0, 0.05, 300))\n","    features = np.asarray(vec).reshape(1, 40, 300)\n","    y_pred = model_w2v.predict(features)\n","    label = lb.inverse_transform([np.argmax(y_pred)])\n","    \n","elif VectorizationWordMethod == 2:\n","    d_tfidf = tfidf_weighed.transform([data])\n","    vocal = tfidf_weighed.vocabulary_\n","    words = data.split()\n","    vecs = []\n","    for i in range(40):\n","        try:\n","            w2v = np.asarray(word2vec.wv[words[i]])\n","        except KeyError:\n","            w2v = np.random.normal(0, 0.05, 300)\n","        except IndexError:\n","            w2v = np.random.normal(0, 0.05, 300)\n","        try:\n","            tfidf = d_tfidf[0, vocal[words[i]]]\n","        except KeyError:\n","            tfidf = np.random.normal(0, 0.05, 1)[0]\n","        except IndexError:\n","            tfidf = np.random.normal(0, 0.05, 1)[0]\n","        vec = tfidf * w2v\n","        vecs.append(vec)\n","    features = np.asarray(vecs).reshape(1, 40, 300)\n","    y_pred = model_wwv.predict(test_data)\n","    label = lb.inverse_transform([np.argmax(y_pred)])\n","else:\n","    pass\n","\n","print(label[0])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["thể thao\n"],"name":"stdout"}]}]}